---
title: "Probability basics for Bayesian analysis"
format: 
  revealjs:
    theme: serif
    navigation-mode: vertical
    incremental: true
    transition: slide
    background-transition: fade
    echo: true
    freeze: auto
---

```{css echo=FALSE}
code {
    font-size: 1.3em;
}
```


```{r echo=FALSE}
library(tidyverse)
library(knitr)
library(gridExtra)
mytheme <- theme(
  legend.text = element_text(size=15),
  legend.title = element_text(size=20),
  axis.title = element_text(size=20),
  axis.text = element_text(size=15))

helper_plot_XY_density <- (function(XY_tibble)
  XY_tibble %>%
  ggplot(aes(X,Y)) 
  + geom_density_2d_filled()
  + mytheme)
```

# A motivating example

To estimate how many times I sneeze per hour, I wait and measure the time till my forth sneeze.

- $X$ - unobserved (parameter) - my rate of sneezes per hour 
- $Y$ - observed - the time till my forth sneeze 

- Prior: $X \sim \Gamma(2,6)$
- Likelihood: $Y \vert _X \sim \Gamma(4,X)$
- Posterior: $X \vert _Y \sim \Gamma(2+4,6+Y)$

## A motivating example - cont.


```{r}
rgamma(1000, 2, 6) %>% qplot()
```
## A motivating example - cont.

```{r}
(tibble(x=seq(0,2,0.01),
        density=dgamma(x, 2, 6)) %>%
  ggplot(aes(x,density)) + mytheme
  + geom_area())
```
## A motivating example - cont.

- Waiting for one event of rate $x$ is distributed $Exp(x),
$Exp(x) = Gamma(1,x)$   (1-"shape", $x$-"rate"),

- Waiting for 4 events of rate x is distributed
$Gamma(4,x)$ (4-shape, $x$-rate).


# Background

[What distinguishes measure theory and probability theory?](https://math.stackexchange.com/questions/118221/what-distinguishes-measure-theory-and-probability-theory)

## Kolmogorov's Foundations of the Theory of Probability
![](./Kolmogorov.jpg)

# Sample space

- $\Omega$ - a set

- Each $\omega$ in $\Omega$ is one possible outcome in our uncertain world.

- In computational uses, $\Omega$ is finite. 

- But when we phrase a statistical model, it is often an infinite mathematical construct.

## Sample space example

```{r}
N <- 100
set.seed(31)
Omega_finite <- tibble(X = rgamma(N, 2, 6),
                Y = rgamma(N, 4, X))

Omega_finite %>% kable()
```
## Sample space example - cont


```{r}
(Omega_finite %>%
  ggplot(aes(X,Y))
  + geom_point(size=5))
```


## Sample space - remarks
- Usually we do not actually care about the elements $\omega$ in $\Omega$.

- We think in terms of events and random variables. 

- The sample space behind them is implicit.

# Events

- Subsets that we care about are called "events". 


## Events example
"all outcomes $\omega$ in $\Omega_{finite}$ in which I waited at least five hours"

```{r}
Omega_finite %>%
filter(Y>5) %>%
kable()
```
## Events example - cont.


```{r}
Omega_finite %>%
filter(Y>5) %>%
nrow()
```




## Event space
- $\mathcal{F}$ - the event space, is the set of events that can be conceptualized in our model of the world.
  - In other words, events where it makes sense to ask whether they occur or not.

- $\mathcal{F}$ is assumed to be a so-called $\sigma$-algebra.
  - It means it has some kind of symmetry that makes it sensible.

## Event space example

In this sample space:
```{r}
tibble(U=c(1,1,2,2),
       V=c(3,4,3,4)) %>%
kable()
```
if $U$ is part of our model of the world, but $V$ is not, then our event space contains events such as $U=1$ but not events such as $V=3$.

## Varying the event space
- When we talk about conditional probability, etc., 
- .. it can always be phrased by conditioning on a different event space.
- .. but we will not use this terminology today.

# Random variables
- A random variable is a function $\Omega \to \mathbb{R}$.
- .. which is "measurable" in the event space.

## Random variables example

```{r}
Omega_finite %>% kable()
```

## Random variables example - cont.
```{r}
Omega_finite$X[13]
```

## Random variables coexist

```{r}
c(Omega_finite$X[13],
  Omega_finite$Y[13])
```

## Random vectors

$(X,Y)$ may be considered a random vector, viewed as a function $\Omega_{finite} \to \mathbb{R}^2$
$$\omega \mapsto (X(\omega), Y(\omega))$$

## Events of random variables

$$Y>5$$
means the subset of $\Omega_{finite}$:
$$\{\omega \in \Omega_{finite} \vert Y(\omega)>5\}$$


```{r}
Y_is_more_than_five <- 
  Omega_finite %>% 
  filter(Y>5)
nrow(Y_is_more_than_five)
```
## Events of random variables - cont.

$$(Y \in [5,9], X<0.3)$$
means the subset of $\Omega_{finite}$:
$$= \{\omega \in \Omega_{finite} \vert Y(\omega) \in [5,9],  X(\omega)<0.3\}$$
$$= \{\omega \in \Omega_{finite} \vert 5 \leq Y(\omega) \leq 9, X(\omega)<0.3\}$$

# Probability

- A probability measure $\mathbb{P}$ is a function from to $\mathcal{F} \to [0,1]$, satisfying [Kolmogorov's axioms](https://www.britannica.com/topic/Foundations-of-the-Theory-of-Probability):
  - $\mathbb{P}(E) \geq 0$ for all $E$ in $\mathcal{F}$
  - $\mathbb{P}(\Omega) = 1$
  - $\mathbb{P}(\bigcup_{i=1,2,...} E_i) = \sum_{i=1,2,...} \mathbb{P}(E_i)$ for pairwise disjoint events

- A sample space with a probability measure is called a probability space.

## Probability example

For our finite example, we may define probabilities proportional to number of outcomes.

```{r}
P_finite <- (function(event)
  nrow(event)/N)

P_finite(Y_is_more_than_five)
```

# Distribution

A random variable $Y: \Omega \to \mathbb{R}$ pushes a probability measure $\mathbb{P}$ over $\Omega$ to a probability measure $P_Y$ over $\mathbb{R}$, called its distribution.

## Distribution example

$$P_X((0,0.3)) =$$
$$\mathbb{P}(X \in (0,0.3)) =$$
$$\mathbb{P}(0 < X < 0.3)$$

## Distribution example - cont.

```{r}
(Omega_finite %>% ggplot() + mytheme
  + geom_histogram(aes(X,..density..), bins=100)
  + geom_segment(x=0, xend=0.3, y=0, yend=0, size=10,
                 color="darkgreen", alpha=0.01)
  + geom_vline(xintercept=c(0,0.3), color="darkgreen"))
```


```{r}
Omega_finite %>% filter(0 < X & X < 0.3) %>% P_finite()
```


## Joint distribution

Similarily, the distribution of a random vector $(X,Y)$ is a probability measure over the plane $\mathbb{R^2}$.

It is also called the *joint distribution* of $X$ and $Y$.

## Joint distribution example

```{r}
(ggplot(Omega_finite) + mytheme
  + geom_point(aes(X,Y), size=5)
  + geom_rect(xmin=0, xmax=0.3, ymin=30, ymax=1000,
              fill="darkgreen", alpha=0.01))
```

# Density

A probability distribution $P_X$ is *absolutely continuous*, if there is a *density* function $f_X$ such that
for every region $D$, $$P_X(D) = \int_D f_X(x) \mathrm{d} x$$

## Density example

```{r}
(tibble(X=seq(0,2,0.01),
        density=dgamma(X,2,6)) %>%
  ggplot(aes(X,density)) + mytheme
  + geom_area(size=3, alpha=0.4)
  + geom_segment(x=0, xend=0.3, y=0, yend=0, size=10,
                 color="darkgreen", alpha=0.01)
  + geom_vline(xintercept=c(0,0.3), color="darkgreen"))
```

## Joint density example

```{r}
(expand.grid(X=seq(0,2,0.01), Y=seq(0,100,1)) %>%
  mutate(density = dgamma(X,2,6) * dgamma(Y,4,X)) %>%
  ggplot(aes(X,Y,z=density)) + mytheme
  + geom_raster(aes(fill=density))
  + geom_rect(xmin=0, xmax=0.3, ymin=30, ymax=1000,
              color="lightgreen", alpha=0.001))
```

# Expectation

- For a random variable $X$ with density $f_X$, it is defined (when the integral is well-defined):
$$\mathbb{E}(X) = \int x f_X(x) \mathrm{d} x$$

- It is actually a special case of a more general notion (not assuming having a density).

- Note that the expectation is determined by the distribution.

## Probability as expectation

- Given an event $A$, we define $1_A$ to be the random variable such that:
  - $1_A(\omega)=1$ if $\omega \in A$ 
  - $1_A(\omega)=0$ if $\omega \notin A$ 

- Then
$$\mathbb{P}(A) = \mathbb{E}(1_A)$$

# Conditioning

## Conditional probability given an event

- Given events $A$, $B$, such that $\mathbb{P}(B)>0$, the conditional probability of $A$ given $B$ is
$$ \mathbb{P}(A|B) = 
\frac
{\mathbb{P}(A \cap B)}
{\mathbb{P}(B)}$$

- When also $\mathbb{P}(A)>0$, we get Bayes' formula:
$$ \mathbb{P}(A|B) = 
\mathbb{P}(B|A)
\frac
{\mathbb{P}(A)}
{\mathbb{P}(B)}$$

## Conditional distribution given an event
- Given a random variable $X:\Omega \to \mathbb{R}$ and an event $B$ such that $\mathbb{P}(B)>0$,
the conditional distribution of $X$ given $B$ is defined by pushing the probability measure $\mathbb{P}(\cdot |B)$ from $\Omega$ to $\mathbb{R}$:
$$P_{X|B} (D) = \mathbb{P}(X \in D|B)$$
for every region $D \subset \mathbb{R}$.

- For example:
$$P_{X|Y>5} ((0,0.3)) = \mathbb{P}(X<0.3 | 5<Y)$$

## Conditional distribution example

```{r}
(Omega_finite %>%
  ggplot(aes(X,..density..)) + mytheme
  + geom_histogram())
```

## Conditional distribution example - cont.


```{r}
(ggplot(Omega_finite) + mytheme
  + geom_point(aes(X,Y), size=5)
  + geom_rect(xmin=0, xmax=2, ymin=10, ymax=1000,
              fill="#00BFC4", alpha=0.01))
```

## Conditional distribution example - cont.

```{r}
(ggplot(Omega_finite) + mytheme
  + geom_point(aes(X,Y,color=Y>10), size=5))
```

## Conditional distribution - cont.
```{r}
(Omega_finite %>%
    ggplot(aes(X,..density..)) + mytheme
  + geom_histogram(aes(fill=factor(Y>10)),
                   position="identity",
                   alpha=0.8))
```

## Conditional expectation given an event
Given a random variable $X:\Omega \to \mathbb{R}$ and an event $B$ such that $\mathbb{P}(B)>0$, the conditional expectation $\mathbb{E}(X \vert B)$ is defined as the expectation of the conditional distribution.


```{r}
Omega_finite %>%
filter(Y>10) %>%
pull(X) %>%
mean
```

## Conditional density given an event
If a conditional distribution has a density, we call it "conditional density".

## Conditional density - cont.

- If $(X,Y)$ is an absolutely continuous random vector whose joint distribution has a density $f_{X,Y}$, and assume that $\mathbb{P}(1.9<Y<2.1)>0$. 

- Then we can look into the conditional density of $X$ given $\mathbb{P}(1.9<Y<2.1)$:
$$f_{X|1.9<Y<2.1}(x) = \frac {\int_{1.9}^{2.1} f_{(X,Y)}(x,y) \mathrm{d}y} {\mathbb{P}(1.9<Y<2.1)}$$
for every $x$.

## Conditional density - cont.

- Indeed, for every $a,b$ such that $a<b$,
$$\mathbb{P}(a<X<b | 1.9<Y<2.1) =$$
$$\frac {\mathbb{P}(a<X<b , 1.9<Y<2.1)} {\mathbb{P}(1.9<Y<2.1)} =$$

$$\frac {\int_a^b \int_{1.9}^{2.1} f_{(X,Y)}(x,y) \mathrm{d}y  \mathrm{d}x} {\mathbb{P}(1.9<Y<2.1)}$$

## Conditional density - cont.
Now what happens when we replace $1.9$ and $2.1$ with numbers which get closer to a limit $y_0$? 

## Conditional density - cont.
Intuitively, for a given $x$ and $y_0$,

$$\frac {\int_{y_0-\delta}^{y_0+\delta} f_{(X,Y)}(x,y) \mathrm{d}y} {\mathbb{P}(y_0-\delta<Y<y_0+\delta)} = $$
$$ \frac {\int_{y_0-\delta}^{y_0+\delta} f_{(X,Y)}(x,y) \mathrm{d}y} {\int_{y_0-\delta}^{y_0+\delta} f_Y(y) \mathrm{d}y}  \approx_{\delta>0, small} $$
$$ \frac { 2 \delta f_{(X,Y)}(x,y_0) \mathrm{d}y} {2 \delta f_Y(y_0)} = $$
$$ \frac { f_{(X,Y)}(x,y_0) \mathrm{d}y} {f_Y(y_0)}$$


## Conditional density given a random variable
- Assume $(X,Y)$ is a random vector whose joint distribution has a density $f_{X,Y}$, then for every $y$ where $f_Y(y)>0$, we can define the conditional density of $X$ given $Y=y$ by
$$f_{X|Y=y}(x) = \frac {f_{X,Y}(x,y)}{f_Y(y)}$$
for every $x$.

- Note this is just a name, remember that $\mathbb{P}(Y=y)=0$ for every $y$.

## Conditional density given a random variable - cont.

- Now, for every $x$ we compose the mapping $y \mapsto f_{X|Y=y}(x)$ with the random variable $Y$:
$$\omega \xrightarrow[]{Y} y \xrightarrow[]{} f_{X|Y=y}(x) = \frac {f_{X,Y}(x,y)}{f_Y(y)}$$

- This way, we get a random variable that we call $f_{X|Y}(x)$:
$$f_{X|Y}(x) = \frac {f_{X,Y}(x,Y)}{f_Y(Y)}$$

- An we may view this as a random density function.

## Conditionint on random variable
- This way, we can also get conditional distribution, probability, and expectation conditioned on a random variable.

- These are all random objects defined in our probability space.

- We can characterize them in a way that generalizes to more general cases (without a density).


# Recommended reading

- G. Jay Kerns, *Introduction to Probability and Statistics Using R*, Third Edition, 2018 [source: IPSUR package](https://cran.r-project.org/web/packages/IPSUR/index.html) version 3.0 (thanks, Blaine Mooers!) -- till subsection 7.3.1

- F.M. Dekking, C. Kraaikamp, H.P. Lopuhaa, and L.E. Meester, *A Modern Introduction to Probability and Statistics -- Understanding Why and How*, [Springer Texts in Statistics, 2005](https://link.springer.com/book/10.1007/1-84628-168-7) -- till about chapter 9
